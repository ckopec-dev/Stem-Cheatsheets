
# ‚öôÔ∏è TensorFlow Tutorial: Batch Normalization Layers

---

## 1. What Is Batch Normalization?

**Batch Normalization (BN)** is a technique to make deep neural networks train faster and more stable.

### ‚úÖ Why Use Batch Normalization

* Reduces **internal covariate shift** (changes in feature distributions across layers).
* Allows **higher learning rates**.
* Acts as a **regularizer**, reducing overfitting.
* Improves convergence speed and accuracy.

---

## 2. Install and Import Dependencies

```bash
pip install tensorflow matplotlib
```

Then in Python:

```python
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
```

---

## 3. Load the Dataset (CIFAR-10 Example)

```python
(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']
```

---

## 4. Build a Baseline CNN (No BatchNorm)

Let‚Äôs start with a simple CNN **without** Batch Normalization:

```python
model_no_bn = models.Sequential([
    layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),
    layers.MaxPooling2D((2,2)),

    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),

    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

model_no_bn.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
```

---

## 5. Build the Same Model **With Batch Normalization**

Now we‚Äôll insert `BatchNormalization` layers **after convolutional or dense layers** (before activation for best results).

```python
model_bn = models.Sequential([
    layers.Conv2D(32, (3,3), use_bias=False, input_shape=(32,32,3)),
    layers.BatchNormalization(),
    layers.Activation('relu'),
    layers.MaxPooling2D((2,2)),

    layers.Conv2D(64, (3,3), use_bias=False),
    layers.BatchNormalization(),
    layers.Activation('relu'),
    layers.MaxPooling2D((2,2)),

    layers.Flatten(),
    layers.Dense(64, use_bias=False),
    layers.BatchNormalization(),
    layers.Activation('relu'),
    layers.Dense(10, activation='softmax')
])

model_bn.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
```

üß† **Note:**
We set `use_bias=False` in layers before BatchNorm because BN already includes bias-like parameters.

---

## 6. Train Both Models

```python
history_no_bn = model_no_bn.fit(
    x_train, y_train,
    epochs=10,
    validation_data=(x_test, y_test),
    verbose=2
)

history_bn = model_bn.fit(
    x_train, y_train,
    epochs=10,
    validation_data=(x_test, y_test),
    verbose=2
)
```

---

## 7. Compare Performance

Plot training vs validation accuracy:

```python
plt.figure(figsize=(10,5))
plt.plot(history_no_bn.history['val_accuracy'], label='Without BatchNorm')
plt.plot(history_bn.history['val_accuracy'], label='With BatchNorm')
plt.xlabel('Epoch')
plt.ylabel('Validation Accuracy')
plt.legend()
plt.show()
```

‚úÖ You should see:

* Faster convergence (higher accuracy early on)
* Better validation performance (less overfitting)

---

## 8. Understanding How Batch Normalization Works

During training, for each mini-batch:
[
\text{BN}(x) = \gamma \frac{x - \mu_{\text{batch}}}{\sqrt{\sigma^2_{\text{batch}} + \epsilon}} + \beta
]
Where:

* ( \mu_{\text{batch}} ) = mean of the batch
* ( \sigma^2_{\text{batch}} ) = variance of the batch
* ( \gamma, \beta ) = trainable scale and shift parameters

During inference, BN uses **running averages** of the mean and variance (not batch statistics).

---

## 9. BatchNorm in Different Places

You can use BatchNorm:

* **After Conv2D**, before activation (most common)
* **After Dense**, before activation
* **Between layers** in deep networks

---

## 10. Advanced Usage: Custom Training Loop

You can also use BN in custom models:

```python
inputs = tf.keras.Input(shape=(32, 32, 3))
x = layers.Conv2D(32, 3, use_bias=False)(inputs)
x = layers.BatchNormalization()(x)
x = layers.ReLU()(x)
x = layers.GlobalAveragePooling2D()(x)
outputs = layers.Dense(10, activation='softmax')(x)

model_custom_bn = tf.keras.Model(inputs, outputs)
model_custom_bn.summary()
```

---

## 11. Key Takeaways

‚úÖ Batch Normalization:

* Stabilizes and speeds up training
* Reduces sensitivity to initialization
* Acts as mild regularization
* Should be applied **before activation** in most cases

---

## 12. Save and Load Model

```python
model_bn.save("cnn_with_batchnorm.h5")
# To load later:
# model_bn = tf.keras.models.load_model("cnn_with_batchnorm.h5")
```

---

## 13. Optional: Combine With Data Augmentation

BatchNorm works beautifully with **data augmentation** to further improve generalization:

```python
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
])
```

Then prepend it to your BatchNorm model.
