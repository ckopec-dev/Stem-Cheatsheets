
# üß† TensorFlow Tutorial: Building Deeper CNNs

---

## 1. What Are Deeper CNNs?

A **deeper CNN** means stacking more convolutional and pooling layers to allow the model to:

* Learn **complex hierarchical features**
* Capture **high-level patterns** (edges ‚Üí shapes ‚Üí objects)
* Achieve higher accuracy on complex datasets

However, deeper networks come with challenges:

* üê¢ **Slower training**
* üî• **Vanishing/exploding gradients**
* üß± **Overfitting**

We‚Äôll address these using:

* **Batch Normalization**
* **Dropout**
* **Residual connections** (optional extension)

---

## 2. Install and Import Dependencies

```bash
pip install tensorflow matplotlib
```

Then in Python:

```python
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
```

---

## 3. Load the Dataset (CIFAR-10)

```python
(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

class_names = ['airplane','automobile','bird','cat','deer',
               'dog','frog','horse','ship','truck']
```

---

## 4. Visualize the Dataset

```python
plt.figure(figsize=(8,8))
for i in range(9):
    plt.subplot(3,3,i+1)
    plt.imshow(x_train[i])
    plt.axis('off')
plt.show()
```

---

## 5. Build a **Deeper CNN Architecture**

Here‚Äôs a custom CNN inspired by **VGG-style** networks.

```python
model = models.Sequential([
    # Block 1
    layers.Conv2D(64, (3,3), padding='same', activation='relu', input_shape=(32,32,3)),
    layers.BatchNormalization(),
    layers.Conv2D(64, (3,3), padding='same', activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2,2)),
    layers.Dropout(0.25),

    # Block 2
    layers.Conv2D(128, (3,3), padding='same', activation='relu'),
    layers.BatchNormalization(),
    layers.Conv2D(128, (3,3), padding='same', activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2,2)),
    layers.Dropout(0.25),

    # Block 3
    layers.Conv2D(256, (3,3), padding='same', activation='relu'),
    layers.BatchNormalization(),
    layers.Conv2D(256, (3,3), padding='same', activation='relu'),
    layers.BatchNormalization(),
    layers.GlobalAveragePooling2D(),  # Instead of Flatten
    layers.Dropout(0.5),

    # Dense Head
    layers.Dense(256, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])
```

---

## 6. Compile the Model

```python
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()
```

This prints the number of layers and parameters ‚Äî expect over **2 million parameters** for this architecture.

---

## 7. Train the Model

```python
history = model.fit(
    x_train, y_train,
    epochs=30,
    batch_size=64,
    validation_data=(x_test, y_test)
)
```

---

## 8. Plot Training Curves

```python
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
```

‚úÖ You should see stable convergence and higher accuracy than shallow CNNs (around **80‚Äì85%** with tuning).

---

## 9. Evaluate on Test Data

```python
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print(f"Test accuracy: {test_acc:.2f}")
```

---

## 10. Make Predictions

```python
import numpy as np

i = 0
prediction = model.predict(x_test[i:i+1])
plt.imshow(x_test[i])
plt.title(f"Predicted: {class_names[np.argmax(prediction)]}, Actual: {class_names[int(y_test[i])]}")
plt.show()
```

---

## 11. Tips for Training Deeper CNNs

| Challenge           | Solution                                   |
| ------------------- | ------------------------------------------ |
| Overfitting         | Use Dropout, Data Augmentation             |
| Vanishing gradients | Use Batch Normalization, Residuals         |
| Slow convergence    | Use learning rate scheduling               |
| Limited compute     | Use smaller batch sizes or mixed precision |

---

## 12. Optional: Add **Residual Connections** (Mini-ResNet Block)

For even better training stability, add residual connections manually:

```python
inputs = tf.keras.Input(shape=(32,32,3))
x = layers.Conv2D(64, 3, padding='same', activation='relu')(inputs)
x = layers.BatchNormalization()(x)
skip = x
x = layers.Conv2D(64, 3, padding='same')(x)
x = layers.BatchNormalization()(x)
x = layers.Add()([x, skip])
x = layers.Activation('relu')(x)
x = layers.GlobalAveragePooling2D()(x)
outputs = layers.Dense(10, activation='softmax')(x)

resnet_model = tf.keras.Model(inputs, outputs)
resnet_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
resnet_model.summary()
```

This is a **mini-ResNet** ‚Äî great for understanding skip connections.

---

## 13. Save and Load the Model

```python
model.save("deeper_cnn_model.h5")
# model = tf.keras.models.load_model("deeper_cnn_model.h5")
```

---

## 14. Key Takeaways

‚úÖ Deeper CNNs learn **complex hierarchical features**
‚úÖ BatchNorm + Dropout make training **stable and regularized**
‚úÖ GlobalAveragePooling2D reduces **parameters and overfitting**
‚úÖ Residual connections enable **much deeper networks**

